#! /usr/bin/env ruby

Main {

  description <<-__
    gnip-expander relays an existing gnip publisher's activites and
    re-publishers a stream with all http/https links published in a shortened
    form (bit.ly, etc) normalized to their long form.

    gnip-expander attempts to setup a keyword filter on the src publisher but
    does not require the publisher to support keyword filters - if the
    publisher does the entire stream is relayed.

    like all gnip.rb client code gnip-expander expects to find your gnip
    account info configured in ~/.gnip.yml in a file that looks like

      username: gnip-username
      password: gnip-password
    
    although you can pass this information in using the command line switches
    of course.

    gnip-expander keeps track of successfully relayed buckets in a directory
    (~/.gnip-expander by default).  although you can pass in the starting
    point with the --timestamp switch gnip-expander is quite good at doing
    'the right thing' with respect to which buckets to process.  in general it
    simply processes all unprocessed buckets as quickly as possible and then
    proceeds to process any new bucket as it appears, polling only when all
    buckets are processed and new ones have yet to appear.
  __
 

  examples <<-__
    . run at the console in verbose mode
        gnip-expander -v4

    . run in the background in daemon mode
        gnip-expander start && tail -F ~/.gnip-expander/log

    . stop a background daemon
        gnip-expander stop

    . restart a background daemon
        gnip-expander restart

    . report the pid of the background daemon or currently running gnip-expander
        gnip-expander pid
  __

  run {
    initialize_program_state

    control_daemon_and_exit_iff_needed!

    ensure_one_instance_is_running

    initialize_gnip

    initialize_src_publisher
    initialize_src_publisher_filter

    initialize_dst_publisher

    initialize_short_url_expanders

    initialize_timestamp

    daemonize_iff_needed do
      foreach_bucket do |bucket|
        publish expanded_activities(bucket)
      end
    end
  }

  argument("mode"){
    description "one of start|stop|restart|pid for daemon control.  all imply --daemon."
    optional
    validate{|mode| %w( start stop restart pid ).include?(mode)}
  }

  option('--timestamp', '-t'){
    description "timestamp to begin relaying activites from"
    argument :required
    cast :time
  }

  option("--gnip-username"){
    description "you gnip username - default from ~/gnip.yml"
    argument :required
  }

  option("--gnip-password"){
    description "you gnip password - default from ~/gnip.yml"
    argument :required
  }

  option("--src-publisher"){
    description "publisher whose posts we want to expand"
    argument :required
    default "twitter"
  }

  option("--daemon", "-D"){
    description "run continuously in the background"
  }

  option("--timeout"){
    description "polling interval in daemon mode"
    argument :required
    default Default.Timeout
    cast :integer
  }

  option("--basedir=basedir"){
    description "base directory for logs, config, pidfile, etc."
    argument :required
    default Default.Basedir
  }

  option('--verbosity', '-v'){
    description 'one of debug(4)|info(3)|warn(2)|error(1)|fatal(0)'
    argument :required
    validate{|verbosity| verbosity.nil? or %w[4 3 2 1 0 debug info warn error fatal].include?(verbosity)}
    default 'info'
  }

  %w[
    basedir
    logfile
    statefile
    state
    gnip
    src_publisher
    src_filter
    dst_publisher
  ].each{|ivar| attr ivar}

  def initialize_program_state
    @basedir = File.expand_path(params['basedir'].value)
    FileUtils.mkdir_p(@basedir)

    @logfile = File.join(@basedir, 'log')
    FileUtils.touch(@logfile) unless File.exist?(@logfile)
    @lockfile = File.join(@basedir, 'lock')
    FileUtils.touch(@lockfile) unless File.exist?(@lockfile)
    @statefile = File.join(@basedir, 'state.yml')
    FileUtils.touch(@statefile) unless File.exist?(@statefile)

    @state = YAML::Store.new(@statefile)

    if((value = params['verbosity'].value))
      verbosity = (4-Integer(value)) rescue Logger.const_get(value.upcase)
      logger.level = verbosity
    end
  end

  def control_daemon_and_exit_iff_needed!
    if(mode=='pid')
      pid = current_pid || fuser_pid
      if process_alive?(pid)
        STDOUT.puts(pid)
        exit!(0)
      else
        exit!(1)
      end
    end

    if(mode=='stop' or mode=='restart')
      stopped =
        catch(:stopped) do
          pid = current_pid || fuser_pid
          throw(:stopped, false) unless pid
          42.times do
            throw(:stopped, pid) unless process_alive?(pid)
            Process.kill('INT', pid) rescue nil
            process_alive?(pid) ? sleep(1) : throw(:stopped, pid)
          end
          nil
        end

      case stopped
        when FalseClass, Numeric
          case mode
            when 'stop'
              STDOUT.puts(stopped) if stopped
              exit!(0)
            when 'restart'
              :nothing
          end
        when NilClass
          abort "failure stopping #{ Program }"
      end
    end
  end

  def mode
    params['mode'].value
  end

  def current_pid
    pid = YAML.load(IO.read(@statefile))['pid'] rescue nil
    pid if process_alive?(pid)
  end

  def fuser_pid
    fuser(@lockfile) || fuser(@logfile)
  end

  def fuser(file)
    stdout = `fuser -f #{ file } 2>/dev/null`.to_s.strip
    pid = Integer(stdout[%r/[^\s]+\s*$/].to_s) rescue nil
    pid = process_alive?(pid)
  end

  def process_alive?(pid)
    pid if(pid and Process.kill(0, pid))
  rescue
    false
  end

  def ensure_one_instance_is_running
    flags = File::APPEND|File::CREAT|File::NONBLOCK|File::RDWR
    fd = begin; open(@lockfile, flags|File::EXCL); rescue; open(@lockfile, flags); end
    unless fd.flock(File::LOCK_EX|File::LOCK_NB)
      pid = current_pid || fuser_pid
      STDERR.puts("#{ Program } @ #{ pid || 'wtf?' }")
      exit(42)
    else
      @state.transaction{ @state['pid'] = Process.pid }
    end
  end

  def initialize_gnip
    @gnip = Gnip
    param = params['gnip-username']
    @gnip.username = param.value if param.given?
    param = params['gnip-password']
    @gnip.password = param.value if param.given?
    abort 'bad gnip config or severed network' unless ping_gnip
  end

  def ping_gnip
    Gnip.ping
  end

  def initialize_src_publisher
    src_publisher = param['src-publisher'].value
    @src_publisher = Gnip.publisher.for(src_publisher)
  end

  def initialize_src_publisher_filter
    keywords = %w( http https )
    name = "keywords--#{ keywords.join('-') }"
    rules = keywords.map{|keyword| {:type => 'keyword', :value => keyword}}
    begin
      @src_filter =
        begin
          @src_publisher.filter.for(name)
        rescue
          @src_publisher.filter.create(name, rules, :full_data => true)
        end
    rescue Object => e
      logger.debug("#{ e.message } (#{ e.class })")
    end
  end

  def initialize_dst_publisher
    name = @src_publisher.name + '-expanded'
    @dst_publisher =
      begin
        Gnip.publisher.for(name, :scope => :my)
      rescue
        begin
          Gnip.publisher.create(name, :scope => :my, :rules => Gnip.publisher.rule.list)
        rescue
          Gnip.publisher.for(name, :scope => :my)
        end
      end
  end

  def initialize_short_url_expanders
    @short_url_expanders = []
    @short_url_expanders << BitlyExpander << TwitterExpander

    @short_url_expanders.each do |expander|
      cache = File.join(@basedir, "#{ expander.name.downcase }.yml")
      if File.exists?(cache)
        begin
          expander.load_cache(cache)
        rescue Object => e
          logger.warn(e)
        end
      end
      at_exit{ expander.dump_cache(cache) rescue nil }
      expander.logger = logger
    end
  end

  def initialize_timestamp
    start_time =
      Time.parse(
        if params['timestamp'].value
          params['timestamp'].value
        else
          @state.transaction(:read_only){ @state['timestamp'] } || Time.at(0)
        end.to_s
      )

    bucket_times = src_bucket_timestamps

    if start_time < bucket_times.first
      return(@timestamp = bucket_times.first)
    end
    if start_time > bucket_times.last
      return(@timestamp = bucket_times.last)
    end
    bucket_times.each do |bucket_time|
      if bucket_time > start_time
        return(@timestamp = bucket_time)
      end
    end

    abort 'no timestamp!' unless @timestamp
  end

  def src_buckets
    @src_publisher.activity_stream.buckets
  end

  def src_bucket_timestamps
    src_buckets.map{|bucket| Time.parse(File.basename(bucket, '.xml'))}.sort
  end

  def daemonize_iff_needed(&block)
    daemonize = (params['daemon'].value or mode=='start')
    return(block.call) unless daemonize
    daemon(&block)
  end

  def daemon(&block)
    r, w = IO.pipe
    fork {
      r.close
      fork {
        Dir.chdir(@basedir)
        pid = Process.pid
        @state.transaction{ @state['pid'] = pid }
        w.puts pid
        w.close
        open(@logfile, 'a+') do |fd|
          fd.sync = true
          STDOUT.reopen(fd)
          STDERR.reopen(fd)
        end
        block.call(nil)
      }
      exit!
    }
    w.close
    buf = r.read
    pid = Integer(buf) rescue nil
    STDOUT.puts(pid || buf)
    exit!
  end

  def foreach_bucket(&block)
    loop do
      begin
        loop do
          bucket_times = src_bucket_timestamps
          if @timestamp < bucket_times.first
            @timestamp += 60
            next
          end
          if @timestamp >= bucket_times.last
            sleep timeout
            next
          end
          break
        end
        @bucket = @timestamp.strftime('%Y%m%d%H%M')
        logger.info{ "processing bucket #{ @bucket.inspect } ..." }
        block.call(@bucket)
        @state.transaction{ @state['timestamp'] = @timestamp }
        @timestamp += 60
      rescue Object => e
        raise if e.is_a?(SystemExit)
        logger.error{ e }
      end
    end
  end

  def timeout
    params['timeout'].value
  end

  def expanded_activities(bucket)
    args =
      if @src_filter
        [{:filter => @src_filter, :bucket => bucket}]
      else
        [{:bucket => bucket}]
      end

    activities = []
    n = -1

    @src_publisher.activity(*args) do |activity|
      n += 1
      if n.zero? and activity.respond_to?(:gnip_resource_uri)
        logger.info{ "src uri #{ activity.gnip_resource_uri.to_s.inspect } ..." }
      end
      next unless has_links?(activity)
      activities << activity
break if activities.size >= 64  # HACK FOR DEBUGGING SPEED
    end

    logger.info{ "expanding #{ activities.size } activities ..." }
    expanded = activities.threadify(32){|activity| expand(activity)}
    logger.info{ "expanded #{ activities.size } activities." }
    expanded
  end

  def has_links? activity
    body = activity.payload.body.to_s #rescue return(false)
    body=~URI.regexp('http') or body=~URI.regexp('https')
  end

  def expand activity
    expand_urls_in activity
    add_meta_source_to activity
    activity
  end
  alias_method 'expanded', 'expand'

  def expand_urls_in activity
    protocols = %w[ http https ]
    body = activity.payload.body.dup
    protocols.each do |protocol|
      body.gsub!(URI.regexp(protocol)){|url| expand_url(url) || url}
    end
    logger.debug{ "expand ->\n - #{ activity.payload.body }\n - #{ body }" }
    activity.payload.body = body
  end

  def expand_url url
    @short_url_expanders.each do |expander|
      return expander.expand(url) if expander.expand?(url)
    end
    false
  end

  def add_meta_source_to activity
    if activity.respond_to?(:gnip_resource_uri)
      meta_source = "gnip(#{ activity.gnip_resource_uri })"
      activity.sources << meta_source
    end
  end

  def publish(activities, publisher = @dst_publisher)
    activities = [activities] unless activities.is_a?(Array)
    return if activities.empty?
    logger.info{ "publishing #{ activities.size } activities ..." }
    publisher.publish(*activities.flatten)
    logger.info{ "published #{ activities.size } activities." }
  end
}


BEGIN {
#
# libs
#
  require 'yaml'
  require 'yaml/store'
  require 'ostruct'
  require 'fileutils'
  require 'time'
  require 'thread'
  require 'sync'
  require 'timeout'
  require 'net/http'
  require 'socket'
  require 'open-uri'
  require 'uri'
  require 'cgi'

  begin
    require 'rubygems'
  rescue LoadError
  end

  begin
    require 'gnip'
  rescue LoadError
    abort "sudo gem install gnip"
  end
  major, minor, teeny = Gnip.version.split(%r/\./).map{|v| v.to_i}
  abort "you need at least gnip-1.0.0" unless major >= 1

  begin
    require 'main'
  rescue LoadError
    abort "sudo gem install main ### your gnip gem should include this the main gem"
  end

#
# global state/context/env
#
  STDERR.sync = STDOUT.sync = true

  ENV['TZ'] = 'UTC'

  trap('INT'){ exit }
  trap('TERM'){ exit }
 
  Program = File.basename(__FILE__)
  def Program.home
    home =
      catch :home do
        ["HOME", "USERPROFILE"].each{|key| throw(:home, ENV[key]) if ENV[key]}
        throw(:home, "#{ ENV['HOMEDRIVE'] }:#{ ENV['HOMEPATH'] }") if ENV["HOMEDRIVE"] and ENV["HOMEPATH"]
        File.expand_path("~") rescue(File::ALT_SEPARATOR ? "C:/" : "/")
      end
    File.expand_path home
  end
 
  Default = OpenStruct.new
  Default.Home = Program.home
  Default.Basedir = File.join(Default.Home, ".#{ Program }")
  Default.Timeout = 60
  Default.Timestamp =
    begin
      YAML.load(IO.read(File.join(Default.Basedir, 'state.yml')))['timestamp'].iso8601
    rescue Object
      'now'
    end
 
  Empty = String.new.freeze

#
# inline some supporting modules
#
  module LRU
    def LRU.cache(*args, &block)
      Cache.new(*args, &block)
    end

    class Cache
      Max = 2 ** 16

      attr_accessor :index
      attr_accessor :max
      attr_accessor :block

      def initialize(options = {}, &block)
        @max = Float(options[:max]||options['max']||Max).to_i
        @block = block
        extend Sync_m
        clear
      end

      def clear
        synchronize(:EX) do
          @index = Hash.new
          @linked_list = LinkedList[]
        end
      end

      def get key, &block
        synchronize(:EX) do
          if @index.has_key?(key)
            node = @index[key]
            @linked_list.remove_node(node)
            @linked_list.push_node(node)
            pair = node.object
            pair.last
          else
            block ||= @block
            raise 'no block!' unless block
            value = block.call(key)
            pair = [key, value]
            @linked_list.push(pair)
            node = @linked_list.last_node
            @index[key] = node
            pair.last
          end
        end
      ensure
        manage_cache
      end

      def put(key, value)
        synchronize(:EX) do
          delete(key)
          get(key){ value }
        end
      end

      def delete(key)
        synchronize(:EX) do
          if @index.has_key?(key)
            node = @index[key]
            pair = node.object
            @linked_list.remove_node(node)
            @index.delete(pair.first)
            pair.last
          end
        end
      end

      def manage_cache
        synchronize(:EX) do
          if size > max
            until size <= max
              node = @linked_list.shift_node
              pair = node.object
              @index.delete(pair.first)
              @linked_list.remove_node(node)
            end
          end
        end
      end

      def size
        synchronize(:SH) do
          @index.size
        end
      end

      def values &block
        synchronize(:SH) do
          result = []
          @linked_list.each do |pair|
            value = pair.last
            block ? block.call(value) : result.push(value)
          end
          block ? self : result
        end
      end

      def keys &block
        synchronize(:SH) do
          result = []
          @linked_list.each do |pair|
            key = pair.first
            block ? block.call(key) : result.push(key)
          end
          block ? self : result
        end
      end

      def to_a
        synchronize(:SH) do
          keys.zip(values)
        end
      end
    end

    class LinkedList
      Node = Struct.new :object, :prev, :next

      include Enumerable

      def LinkedList.[](*args)
        new(*args)
      end

      attr :size

      def initialize(*args)
        replace(args)
      end

      def replace(args=nil)
        @first = Node.new
        @last = Node.new
        @first.next = @last
        @last.prev = @first
        @size = 0
        args = args.to_a
        args.to_a.each{|arg| push(arg)} unless args.empty?
        self
      end

      def first
        not_empty! and @first.next.object
      end

      def first_node
        not_empty! and @first.next
      end

      def last
        not_empty! and @last.prev.object
      end

      def last_node
        not_empty! and @last.prev
      end

      def not_empty!
        @size <= 0 ? raise('empty') : @size
      end

      def push(object)
        push_node(Node.new(object, @last.prev, @last)).object
      end

      def push_node(node)
        @last.prev.next = node
        @last.prev = node
        @size += 1
        node
      end

      def <<(object)
        push(object)
        self
      end

      def pop
        pop_node.object
      end

      def pop_node
        raise('empty') if @size <= 0
        node = @last.prev
        node.prev.next = @last
        @last.prev = node.prev
        @size -= 1
        node
      end

      def unshift(object)
        unshift_node(Node.new(object, @first, @first.next)).object
      end

      def unshift_node(node)
        @first.next.prev = node
        @first.next = node
        @size += 1
        node
      end

      def shift
        shift_node.object
      end

      def shift_node
        raise('empty') if @size <= 0
        node = @first.next
        node.next.prev = @first
        @first.next = node.next
        @size -= 1
        node
      end

      def remove_node(node)
        not_empty!
        node.prev.next = node.next
        node.next.prev = node.prev
        node
      end

      def each_node
        node = @first.next
        while node != @last
          yield node
          node = node.next
        end
        self
      end

      def each
        each_node{|node| yield node.object}
      end

      def reverse_each_node
        node = @last
        loop do
          yield node
          node = node.prev
          if ! node
            break
          end
        end
        self
      end

      def reverse_each
        reverse_each_node{|node| yield node.object}
      end

      alias_method '__inspect__', 'inspect' unless instance_methods.include?('__inspect__')

      def inspect
        to_a.inspect
      end
    end

  end

  module Curl
    attr_accessor :timeout
    @timeout = 60

    def get(uri, options = {})
      uri = URI.parse(uri)
      uri.query = query_string_for(options)
      uri = uri.to_s

      e = nil
      42.times do
        begin
          return open(uri.to_s){|socket| socket.read}.to_s.strip
        rescue EOFError, Timeout::Error => e
          nil
        end
      end
      raise(e || "failed on #{ uri.inspect }")
    end

    def query_string_for(options = {})
      return nil if options.empty?
      options.to_a.map{|k,v| [escape(k), escape(v)].join('=')}.join('&')
    end

    def escape(string)
      string.to_s.gsub(/([^ a-zA-Z0-9_.-]+)/n) do
        '%' + $1.unpack('H2' * $1.size).join('%').upcase
      end.tr(' ', '+')
    end

    extend self
  end

  module AbstractExpander
    attr_accessor :logger

    def expand?(uri)
      raise NotImplementedError
    end

    def expand!(url)
      raise NotImplementedError
    end

    def expand(url)
      expanded = cache.get(url)
    end

    def cache
      expander = self
      @cache ||= LRU.cache{|url| expander.expand!(url) }
    end

    def dump_cache(cachefile)
      logger.debug{ "dumping cache #{ cachefile }" } if logger
      YAML::Store.new(cachefile).transaction do |ystore|
        ystore['data'] = cache.to_a
      end
    end

    def load_cache(cachefile)
      logger.debug{ "loading cache #{ cachefile }" } if logger
      YAML::Store.new(cachefile).transaction do |ystore|
        if((data = ystore['data']))
          data.each do |key, val|
            cache.put(key, val)
          end
        end
      end
    end
  end

  module BitlyExpander
    include AbstractExpander

    def expand?(url)
      !!(url.to_s =~ %r|http(s)?://bit\.ly|)
    end

    def expand!(url)
      begin
        location(url)
      rescue Object => e
        raise if e.is_a?(SystemExit)
        logger.error{ e }
        url
      end
    end

    def location(url)
      head(url)['location']
    end

    def head(url)
      uri = URI.parse(url.to_s)
      e = nil
      42.times do
        begin
          Net::HTTP.start(uri.host, uri.port||80) do |http|
            return http.head(uri.path)
          end
        rescue Timeout::Error => e
          nil
        end
      end
      raise(e || "failed on #{ url.inspect }")
    end

    extend self
  end

  module TwitterExpander
    include AbstractExpander

    Uri = "http://search.twitter.com/hugeurl"

    def expand?(uri)
      true
    end

    def expand!(url, &block)
      result = url
      begin
        result = Curl.get(Uri, :url => url)
        result = url if result.empty?
        result = url unless(result =~ %r|^http://|)
        result
      rescue Object => e
        raise if e.is_a?(SystemExit)
        logger.error(e) if logger unless(e.is_a?(OpenURI::HTTPError) and e.message=~/500 Internal Server Error/)
        url
      end
    ensure
      logger.debug{ "expand!(#{ url.to_s.inspect } => #{ result.inspect })" } if logger
    end

    extend self
  end
}
